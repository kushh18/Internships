{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  <center> Speech Emotion Recognition <center>","metadata":{"id":"2N7tAA_UxlLo"}},{"cell_type":"markdown","source":"Datasets contains 8 types of emotions:\n\nEmotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).","metadata":{"id":"6Q8mrytWxlMD"}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{"id":"n6gxm0C5xlMF"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport sys\nimport random\n\n# librosa is a Python library for analyzing audio and music.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\ntf.random.set_seed(30)\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, GRU, LSTM, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\nfrom keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n#from keras.layers.recurrent import LSTM\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras import optimizers\nfrom keras.optimizers import adam_v2\nfrom keras.layers import Dropout\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","id":"ASukAAhaxlMG","execution":{"iopub.status.busy":"2022-12-08T16:51:38.875895Z","iopub.execute_input":"2022-12-08T16:51:38.876283Z","iopub.status.idle":"2022-12-08T16:51:38.885783Z","shell.execute_reply.started":"2022-12-08T16:51:38.876252Z","shell.execute_reply":"2022-12-08T16:51:38.884715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation\nCreating a dataframe storing all emotions of the data in dataframe with their paths. I will use this dataframe to extract features for our model training.","metadata":{"id":"XAHYQyEQxlML"}},{"cell_type":"code","source":"# Paths for data.\nRavdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"","metadata":{"id":"Sq7Q1ZWoxlMM","execution":{"iopub.status.busy":"2022-12-08T16:42:22.720762Z","iopub.execute_input":"2022-12-08T16:42:22.721159Z","iopub.status.idle":"2022-12-08T16:42:22.7269Z","shell.execute_reply.started":"2022-12-08T16:42:22.721129Z","shell.execute_reply":"2022-12-08T16:42:22.725468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  <center> 1. Ravdess Dataframe <center>\nHere is the filename identifiers as per the official RAVDESS website:\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).","metadata":{"id":"XuUEW_46xlMN"}},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\nfile_emotion = []\nfile_statement = []\nfile_path = []\nfor dir in ravdess_directory_list:\n    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n    actor = os.listdir(Ravdess + dir)\n    for file in actor:\n        part = file.split('.')[0]\n        part = part.split('-')\n        # third part in each file represents the emotion associated to that auido.\n        file_emotion.append(int(part[2]))\n        file_statement.append(int(part[4]))\n        file_path.append(Ravdess + dir + '/' + file)\n        \n# dataframe for emotion of files\n#emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\n#path_df = pd.DataFrame(file_path, columns=['Path'])\n#Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\nRavdess_df = pd.DataFrame({\"Emotions\":file_emotion, \"Statement\":file_statement, \"Path\":file_path})\n\n# changing integers to actual emotions.\nRavdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df.head()","metadata":{"id":"0it-c-u8xlMP","outputId":"180b0fd1-fd47-45bc-8171-c9c36ab53e69","execution":{"iopub.status.busy":"2022-12-08T16:43:12.89962Z","iopub.execute_input":"2022-12-08T16:43:12.900567Z","iopub.status.idle":"2022-12-08T16:43:13.296582Z","shell.execute_reply.started":"2022-12-08T16:43:12.90052Z","shell.execute_reply":"2022-12-08T16:43:13.295491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ravdess_df.shape","metadata":{"id":"5UBCRVsNXTpX","outputId":"81c9fbbb-1b5e-4cb6-82bc-494c8b9877b4","execution":{"iopub.status.busy":"2022-12-08T16:43:18.35487Z","iopub.execute_input":"2022-12-08T16:43:18.355312Z","iopub.status.idle":"2022-12-08T16:43:18.363248Z","shell.execute_reply.started":"2022-12-08T16:43:18.355272Z","shell.execute_reply":"2022-12-08T16:43:18.362308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ravdess_df.groupby(['Emotions','Statement']).size().unstack()","metadata":{"id":"B5H5JvrEfnRB","outputId":"75518390-bead-4041-f24c-25e02e0a2557","execution":{"iopub.status.busy":"2022-12-08T16:43:22.420004Z","iopub.execute_input":"2022-12-08T16:43:22.420454Z","iopub.status.idle":"2022-12-08T16:43:22.45331Z","shell.execute_reply.started":"2022-12-08T16:43:22.4204Z","shell.execute_reply":"2022-12-08T16:43:22.452405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a copy\ndata_path = Ravdess_df[['Emotions','Path']].copy()\ndata_path.to_csv(\"data_path.csv\",index=False)\ndata_path.head()","metadata":{"id":"hquZquBPxlMX","outputId":"0eeee2ce-befd-458b-abc7-576ae17b1939","execution":{"iopub.status.busy":"2022-12-08T16:43:55.700452Z","iopub.execute_input":"2022-12-08T16:43:55.700887Z","iopub.status.idle":"2022-12-08T16:43:55.722809Z","shell.execute_reply.started":"2022-12-08T16:43:55.700852Z","shell.execute_reply":"2022-12-08T16:43:55.721739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualisation and Exploration","metadata":{"id":"Z7fQyqlkxlMY"}},{"cell_type":"code","source":"plt.title('Count of Emotions', size=16)\nsns.countplot(data_path.Emotions)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()","metadata":{"id":"RWwIpUbcxlMZ","outputId":"0ca5ee27-5321-471c-86fd-64d3840d1e42","execution":{"iopub.status.busy":"2022-12-08T16:44:07.085786Z","iopub.execute_input":"2022-12-08T16:44:07.08618Z","iopub.status.idle":"2022-12-08T16:44:07.361381Z","shell.execute_reply.started":"2022-12-08T16:44:07.086147Z","shell.execute_reply":"2022-12-08T16:44:07.360366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting waveplots and spectograms for audio signals.","metadata":{"id":"QqJdNWWdxlMb"}},{"cell_type":"code","source":"def create_waveplot(data, sr):\n    plt.figure(figsize=(10, 3))\n    plt.title('Waveplot for audio', size=15)\n    librosa.display.waveshow(data, sr=sr)\n    plt.show()\n\ndef create_spectrogram(data, sr):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for audio', size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')   \n    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar()","metadata":{"id":"XQi2jelvxlMc","execution":{"iopub.status.busy":"2022-12-08T16:54:38.138383Z","iopub.execute_input":"2022-12-08T16:54:38.139008Z","iopub.status.idle":"2022-12-08T16:54:38.153461Z","shell.execute_reply.started":"2022-12-08T16:54:38.138952Z","shell.execute_reply":"2022-12-08T16:54:38.152355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-01-01-01-01-01.wav\"\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate)\nAudio(path)","metadata":{"id":"giy_1PAxDHlK","outputId":"6398ba71-df18-4ed2-f249-ac5b79f950bb","execution":{"iopub.status.busy":"2022-12-08T16:54:41.426987Z","iopub.execute_input":"2022-12-08T16:54:41.42738Z","iopub.status.idle":"2022-12-08T16:54:41.938938Z","shell.execute_reply.started":"2022-12-08T16:54:41.427348Z","shell.execute_reply":"2022-12-08T16:54:41.937885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='fear'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate)\ncreate_spectrogram(data, sampling_rate)\nAudio(path)","metadata":{"id":"Hsz5diIAxlMd","outputId":"c1fe25ab-c3d8-4d6c-91ac-05172b463633","execution":{"iopub.status.busy":"2022-12-08T16:55:53.57241Z","iopub.execute_input":"2022-12-08T16:55:53.572787Z","iopub.status.idle":"2022-12-08T16:55:54.654519Z","shell.execute_reply.started":"2022-12-08T16:55:53.572756Z","shell.execute_reply":"2022-12-08T16:55:54.653579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='angry'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate)\ncreate_spectrogram(data, sampling_rate)\nAudio(path)","metadata":{"id":"xLTOd6GWxlMe","outputId":"5900a02d-7b1d-4c4b-abda-56b29f50ac55","execution":{"iopub.status.busy":"2022-12-08T16:56:03.514803Z","iopub.execute_input":"2022-12-08T16:56:03.515171Z","iopub.status.idle":"2022-12-08T16:56:04.619705Z","shell.execute_reply.started":"2022-12-08T16:56:03.51514Z","shell.execute_reply":"2022-12-08T16:56:04.618677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='sad'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate)\ncreate_spectrogram(data, sampling_rate)\nAudio(path)","metadata":{"id":"TsYeiPKJxlMf","outputId":"6bbd9dda-2d71-4e21-984d-eed9bc2fc78c","execution":{"iopub.status.busy":"2022-12-08T16:56:13.093065Z","iopub.execute_input":"2022-12-08T16:56:13.093631Z","iopub.status.idle":"2022-12-08T16:56:14.135885Z","shell.execute_reply.started":"2022-12-08T16:56:13.093587Z","shell.execute_reply":"2022-12-08T16:56:14.134845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='happy'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate)\ncreate_spectrogram(data, sampling_rate)\nAudio(path)","metadata":{"id":"6xfFpClUxlMj","outputId":"1ec832f3-9d21-48a1-e219-bc90fa4d8c2d","execution":{"iopub.status.busy":"2022-12-08T16:56:21.987308Z","iopub.execute_input":"2022-12-08T16:56:21.987704Z","iopub.status.idle":"2022-12-08T16:56:23.044562Z","shell.execute_reply.started":"2022-12-08T16:56:21.987658Z","shell.execute_reply":"2022-12-08T16:56:23.043625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation\n\n- The objective is to make our model invariant to those perturbations and enhace its ability to generalize.\n- Using Random transformations and augmenting data before training.","metadata":{"id":"T4lh_5jhxlMk"}},{"cell_type":"code","source":"def noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.85):\n    return librosa.effects.time_stretch(data, rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n\n# taking any example and checking for techniques.\npath = np.array(data_path.Path)[1]\ndata, sample_rate = librosa.load(path)","metadata":{"id":"bpDLPabCxlMl","execution":{"iopub.status.busy":"2022-12-08T16:56:34.477203Z","iopub.execute_input":"2022-12-08T16:56:34.477582Z","iopub.status.idle":"2022-12-08T16:56:34.629652Z","shell.execute_reply.started":"2022-12-08T16:56:34.47755Z","shell.execute_reply":"2022-12-08T16:56:34.62872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Simple Audio","metadata":{"id":"64xEh_fNxlMl"}},{"cell_type":"code","source":"plt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=data, sr=sample_rate)\nAudio(path)","metadata":{"id":"uogtynrmxlMm","outputId":"4f83dfd3-5fc8-4887-958d-16de83aba89a","execution":{"iopub.status.busy":"2022-12-08T16:56:48.729888Z","iopub.execute_input":"2022-12-08T16:56:48.730303Z","iopub.status.idle":"2022-12-08T16:56:49.376878Z","shell.execute_reply.started":"2022-12-08T16:56:48.730268Z","shell.execute_reply":"2022-12-08T16:56:49.375699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Noise Injection","metadata":{"id":"DWmHnugaxlMm"}},{"cell_type":"code","source":"x = noise(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"id":"xFvUl5A0xlMn","outputId":"6c471471-1168-43fc-988c-1f381061f7f6","execution":{"iopub.status.busy":"2022-12-08T16:57:01.038205Z","iopub.execute_input":"2022-12-08T16:57:01.039286Z","iopub.status.idle":"2022-12-08T16:57:01.830083Z","shell.execute_reply.started":"2022-12-08T16:57:01.039234Z","shell.execute_reply":"2022-12-08T16:57:01.828585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Stretching","metadata":{"id":"aVsJHBHrxlMo"}},{"cell_type":"code","source":"x = stretch(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"id":"yk6jOtsnxlMo","outputId":"783cc6bd-2db7-4394-9925-c4011d2bb385","execution":{"iopub.status.busy":"2022-12-08T16:57:09.157651Z","iopub.execute_input":"2022-12-08T16:57:09.158281Z","iopub.status.idle":"2022-12-08T16:57:10.077611Z","shell.execute_reply.started":"2022-12-08T16:57:09.158234Z","shell.execute_reply":"2022-12-08T16:57:10.076556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Shifting","metadata":{"id":"XJSM1GGcxlMp"}},{"cell_type":"code","source":"x = shift(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"id":"9-PQG4oGxlMp","outputId":"d8660324-d798-43ba-ff2e-e80beaf9aa1f","execution":{"iopub.status.busy":"2022-12-08T16:57:41.41223Z","iopub.execute_input":"2022-12-08T16:57:41.412593Z","iopub.status.idle":"2022-12-08T16:57:41.903028Z","shell.execute_reply.started":"2022-12-08T16:57:41.412562Z","shell.execute_reply":"2022-12-08T16:57:41.901956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. Pitch","metadata":{"id":"YjnmHXPNxlMp"}},{"cell_type":"code","source":"x = pitch(data, sample_rate)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"id":"YkO5Qqc1xlMq","outputId":"b0facdcd-4d44-45bf-8ac4-3dddc1efc635","execution":{"iopub.status.busy":"2022-12-08T16:57:49.277198Z","iopub.execute_input":"2022-12-08T16:57:49.277667Z","iopub.status.idle":"2022-12-08T16:57:49.8021Z","shell.execute_reply.started":"2022-12-08T16:57:49.277626Z","shell.execute_reply":"2022-12-08T16:57:49.800897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction\nWith the help of the sample rate and the sample data, one can perform several transformations on it to extract valuable features out of it. Extracting MFCC for the current experiment.\n\n> MFCCs Mel Frequency Cepstral Coefficients form a cepstral representation where the frequency bands are not linear but distributed according to the mel-scale.","metadata":{"id":"V3ggu8PIxlMq"}},{"cell_type":"code","source":"def extract_features(data):\n    # MFCC\n    mfcc = librosa.feature.mfcc(y=data, sr=sample_rate)\n    result = mfcc\n    return result\n\n# funtion to transform audio\ndef transform_audio(data, fns):\n    fn = random.choice(fns)\n    if fn == pitch:\n        fn_data = fn(data, sampling_rate)\n    elif fn == \"None\":\n        fn_data = data\n    elif fn in [noise, stretch]:\n        fn_data = fn(data)\n    else:\n        fn_data = data\n    return fn_data\n\n\ndef get_features(path):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n\n\n    #randomly transforming audio data\n    fns = [noise, pitch, \"None\"]\n\n    fn1_data = transform_audio(data, fns)\n    fn2_data = transform_audio(fn1_data, fns)\n    res1 = extract_features(fn2_data)\n    result = [np.array(res1[:,:108])]\n\n    fn1_data = transform_audio(data, fns)\n    fn2_data = transform_audio(fn1_data, fns)\n    res2 = extract_features(fn2_data)[:,:108]\n    result.append(res2)#np.vstack((result, res2)) # stacking vertically\n\n    fn1_data = transform_audio(data, fns)\n    fn2_data = transform_audio(fn1_data, fns)\n    res3 = extract_features(fn2_data)[:,:108]\n    result.append(res3)#np.vstack((result, res3)) # stacking vertically\n\n    return result","metadata":{"id":"HZ5Clnq1qQpK","execution":{"iopub.status.busy":"2022-12-08T17:02:24.028669Z","iopub.execute_input":"2022-12-08T17:02:24.029057Z","iopub.status.idle":"2022-12-08T17:02:24.039982Z","shell.execute_reply.started":"2022-12-08T17:02:24.029028Z","shell.execute_reply":"2022-12-08T17:02:24.038951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-01-01-01-01-01.wav\"\ndata, sampling_rate = librosa.load(path, duration=2.5, offset=0.6)\n#create_waveplot(data, sampling_rate, emotion)\n#create_spectrogram(data, sampling_rate, emotion)\nAudio(path)\nxx1 = librosa.feature.zero_crossing_rate(data)\nstft = np.abs(librosa.stft(data))\nxx2 = librosa.feature.chroma_stft(S=stft, sr=sample_rate)\nxx3 = librosa.feature.mfcc(y=data, sr=sample_rate)\nxx4 = librosa.feature.rms(y=data)\nxx5 = librosa.feature.melspectrogram(y=data, sr=sample_rate)\nxx1.shape, xx2.shape, xx3.shape, xx4.shape, xx5.shape, np.append(xx1,xx2,axis=0).shape\n\n#xx = extract_features(data)\n#fns = [noise, stretch, pitch]\n#xx1 = extract_features(stretch(data))\n#xx.shape, xx1.shape\n#xx = get_features(path)\n#len(xx)","metadata":{"id":"xLdonKCfh7nS","outputId":"4143cc7a-a33a-40da-d966-598624c85af2","execution":{"iopub.status.busy":"2022-12-08T17:03:40.758206Z","iopub.execute_input":"2022-12-08T17:03:40.75858Z","iopub.status.idle":"2022-12-08T17:03:40.902839Z","shell.execute_reply.started":"2022-12-08T17:03:40.758548Z","shell.execute_reply":"2022-12-08T17:03:40.901583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, Y = [], []\nfor path, emotion in zip(data_path.Path.to_list(), data_path.Emotions.to_list()):\n    feature = get_features(path)\n    for ele in feature:\n        if ele.shape == (20, 108):\n            X.append(ele)\n            # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n            Y.append(emotion)\n        else:\n            print(ele.shape)","metadata":{"id":"BrMhAvw1xlMr","execution":{"iopub.status.busy":"2022-12-08T17:04:59.75289Z","iopub.execute_input":"2022-12-08T17:04:59.753888Z","iopub.status.idle":"2022-12-08T17:16:26.332437Z","shell.execute_reply.started":"2022-12-08T17:04:59.75384Z","shell.execute_reply":"2022-12-08T17:16:26.330859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X), len(Y), data_path.Path.shape","metadata":{"id":"1jndiwDJxlMr","outputId":"e37e2219-dda9-4f7f-e29f-3b36a2038c2c","execution":{"iopub.status.busy":"2022-12-08T17:20:14.022165Z","iopub.execute_input":"2022-12-08T17:20:14.022515Z","iopub.status.idle":"2022-12-08T17:20:14.030788Z","shell.execute_reply.started":"2022-12-08T17:20:14.022485Z","shell.execute_reply":"2022-12-08T17:20:14.028632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfm = pd.DataFrame()\nxx = pd.Series(X)\nfor i in range(20):\n    dfm[i] = [k[i] for k in xx]","metadata":{"id":"Q6OthEAB7Ps0","outputId":"be35923c-596b-48d7-fc0f-f5d0964a22e8","execution":{"iopub.status.busy":"2022-12-08T17:22:14.607732Z","iopub.execute_input":"2022-12-08T17:22:14.608234Z","iopub.status.idle":"2022-12-08T17:22:14.79184Z","shell.execute_reply.started":"2022-12-08T17:22:14.608191Z","shell.execute_reply":"2022-12-08T17:22:14.790762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Features = dfm.copy()\nFeatures['labels'] = Y\n#Features.to_csv('features.csv', index=False)\nFeatures.head()","metadata":{"id":"VLz1fIj6xlMs","outputId":"5f5eb7b6-dfea-4203-9ee9-61d30a8fc2da","execution":{"iopub.status.busy":"2022-12-08T17:22:20.652282Z","iopub.execute_input":"2022-12-08T17:22:20.65267Z","iopub.status.idle":"2022-12-08T17:22:20.78015Z","shell.execute_reply.started":"2022-12-08T17:22:20.652635Z","shell.execute_reply":"2022-12-08T17:22:20.779244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(X)\nY = np.array(Y)\nX.shape, Y.shape","metadata":{"id":"JcXX1e2mIaU-","outputId":"2be3ad0b-bfd3-46b1-a907-0577e9090773","execution":{"iopub.status.busy":"2022-12-08T17:22:24.542249Z","iopub.execute_input":"2022-12-08T17:22:24.54286Z","iopub.status.idle":"2022-12-08T17:22:24.58891Z","shell.execute_reply.started":"2022-12-08T17:22:24.542811Z","shell.execute_reply":"2022-12-08T17:22:24.587759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{"id":"GHZvDJeRxlMt"}},{"cell_type":"code","source":"# As this is a multiclass classification problem onehotencoding our Y.\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","metadata":{"id":"puvDJaeZxlMu","execution":{"iopub.status.busy":"2022-12-08T17:22:28.437255Z","iopub.execute_input":"2022-12-08T17:22:28.437874Z","iopub.status.idle":"2022-12-08T17:22:28.452022Z","shell.execute_reply.started":"2022-12-08T17:22:28.437812Z","shell.execute_reply":"2022-12-08T17:22:28.450863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=21, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"id":"ik3udlEZxlMv","outputId":"2d566994-99ec-4d92-b810-06234173c8b1","execution":{"iopub.status.busy":"2022-12-08T17:22:32.836609Z","iopub.execute_input":"2022-12-08T17:22:32.837609Z","iopub.status.idle":"2022-12-08T17:22:32.871108Z","shell.execute_reply.started":"2022-12-08T17:22:32.837565Z","shell.execute_reply":"2022-12-08T17:22:32.870028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making data compatible to model.\nx_train = np.expand_dims(x_train, axis=3)\nx_train = np.swapaxes(x_train, 1, 2)\nx_train = np.expand_dims(x_train, axis=3)\n\nx_test = np.expand_dims(x_test, axis=3)\nx_test = np.swapaxes(x_test, 1, 2)\nx_test = np.expand_dims(x_test, axis=3)\n\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"id":"FCeMr-qNxlMw","outputId":"2ec28802-670c-42b7-9950-c1dba44df1cd","execution":{"iopub.status.busy":"2022-12-08T17:22:35.821728Z","iopub.execute_input":"2022-12-08T17:22:35.822099Z","iopub.status.idle":"2022-12-08T17:22:35.832338Z","shell.execute_reply.started":"2022-12-08T17:22:35.822068Z","shell.execute_reply":"2022-12-08T17:22:35.831218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{"id":"edUR-FenxlMw"}},{"cell_type":"code","source":"input_shape = x_train.shape[1:]\ninput_shape #(108, 162, 1, 1)","metadata":{"id":"AyebX-A-MbZO","outputId":"c3bc60c1-9250-4563-c533-29dd16817c48","execution":{"iopub.status.busy":"2022-12-08T17:22:39.13163Z","iopub.execute_input":"2022-12-08T17:22:39.132349Z","iopub.status.idle":"2022-12-08T17:22:39.13872Z","shell.execute_reply.started":"2022-12-08T17:22:39.132313Z","shell.execute_reply":"2022-12-08T17:22:39.137742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=Sequential()\n\nmodel.add(TimeDistributed(Conv1D(16, 3, padding='same', activation='relu'),\n                            input_shape=input_shape))\nmodel.add(TimeDistributed(BatchNormalization()))\n#model.add(TimeDistributed(MaxPooling2D((2,1))))\n\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(LSTM(32))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=8, activation='softmax'))\n\nmodel.summary()","metadata":{"id":"-paGmYWxxlMw","outputId":"ea4d4185-0e7c-4420-918e-66aed31c2e31","execution":{"iopub.status.busy":"2022-12-08T17:22:42.31644Z","iopub.execute_input":"2022-12-08T17:22:42.316827Z","iopub.status.idle":"2022-12-08T17:22:48.563737Z","shell.execute_reply.started":"2022-12-08T17:22:42.316796Z","shell.execute_reply":"2022-12-08T17:22:48.562694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.optimizers import adam_v2\noptimizer = adam_v2.Adam()\nmodel.compile(optimizer=optimizer,\n              loss = 'categorical_crossentropy',\n              metrics=['accuracy'])\n\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=4, min_lr=0.0000001)\n\nepochs = 100\nhistory = model.fit(x_train, y_train, batch_size=128, epochs=epochs, validation_data=(x_test, y_test), callbacks=[rlrp])","metadata":{"id":"Wn0XtPjtuoRs","outputId":"3965760e-e03f-449b-a7a2-f4a5a3231f3f","execution":{"iopub.status.busy":"2022-12-08T17:22:53.351602Z","iopub.execute_input":"2022-12-08T17:22:53.352835Z","iopub.status.idle":"2022-12-08T17:23:45.088235Z","shell.execute_reply.started":"2022-12-08T17:22:53.352789Z","shell.execute_reply":"2022-12-08T17:23:45.087339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(100)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","metadata":{"id":"fnvY5v8YxlMy","outputId":"89e94581-8aa3-4530-cb37-b232b424c7d7","execution":{"iopub.status.busy":"2022-12-08T17:24:08.9326Z","iopub.execute_input":"2022-12-08T17:24:08.933398Z","iopub.status.idle":"2022-12-08T17:24:09.797363Z","shell.execute_reply.started":"2022-12-08T17:24:08.933359Z","shell.execute_reply":"2022-12-08T17:24:09.796415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting on test data.\npred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)","metadata":{"id":"clMBP1eMxlMy","outputId":"b4249852-3bf8-4f5e-f595-22a1633b2d00","execution":{"iopub.status.busy":"2022-12-08T17:24:15.651597Z","iopub.execute_input":"2022-12-08T17:24:15.652308Z","iopub.status.idle":"2022-12-08T17:24:16.159101Z","shell.execute_reply.started":"2022-12-08T17:24:15.652271Z","shell.execute_reply":"2022-12-08T17:24:16.157987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","metadata":{"id":"AZDcHwKKxlMz","outputId":"74faf4e5-c086-4db2-9013-a3408a5cea22","execution":{"iopub.status.busy":"2022-12-08T17:24:20.126709Z","iopub.execute_input":"2022-12-08T17:24:20.12731Z","iopub.status.idle":"2022-12-08T17:24:20.150098Z","shell.execute_reply.started":"2022-12-08T17:24:20.12726Z","shell.execute_reply":"2022-12-08T17:24:20.148559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","metadata":{"id":"m64picU0xlMz","outputId":"7001f96d-4f7f-45c2-9aac-427ea3eaa764","execution":{"iopub.status.busy":"2022-12-08T17:24:23.52644Z","iopub.execute_input":"2022-12-08T17:24:23.526816Z","iopub.status.idle":"2022-12-08T17:24:24.029591Z","shell.execute_reply.started":"2022-12-08T17:24:23.526784Z","shell.execute_reply":"2022-12-08T17:24:24.028598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"id":"zQHxyYOzxlM0","outputId":"8cd79e92-34f8-4552-b6b2-8d6059277503","execution":{"iopub.status.busy":"2022-12-08T17:24:28.252089Z","iopub.execute_input":"2022-12-08T17:24:28.252442Z","iopub.status.idle":"2022-12-08T17:24:28.273316Z","shell.execute_reply.started":"2022-12-08T17:24:28.252412Z","shell.execute_reply":"2022-12-08T17:24:28.272394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Obtained is an acceptalbe model but poorly regularised. There is a clear case of overfitting here.\n- Model is more accurate in predicting surprise, angry and calm emotions as audio corresponding to these emotions are much different to other emotions in a lot of ways like pitch, speed etc..\n- Ooverall 65% accuracy on our test data and its decent but we can improve it more by applying more augmentation techniques and using other feature extraction methods.\n- Perhaps, better augmentation, realtime/on the go data augmentation could have produced a more regularised result.\n- Different architectures and normalization methods can also be explored. ","metadata":{"id":"WaMiye-pxlM0"}},{"cell_type":"code","source":"","metadata":{"id":"kZ3_WP5Wzdrk"},"execution_count":null,"outputs":[]}]}